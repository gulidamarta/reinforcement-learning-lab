{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise 4 - DDPG\n",
    "In this exercise we will train our own agent using [Deep Deterministic Policy Gradient](https://arxiv.org/pdf/1509.02971.pdf) to safely land a lunarlander from [Gymnasium environments](https://gymnasium.farama.org/environments/box2d/lunar_lander/). Previously we looked at the DQN algorithm which works for discrete action spaces. Many real world scenarious, such as robotics or self-driving cars, require the agent/controller to perform on a continuous action space. So it is time to move from discrete world to a continuous world! \n",
    "\n",
    "\n",
    "<img src=\"resources/lunar_lander.gif\" alt=\"Gymnasium\" width=\"20%\"/>\n",
    "\n",
    "<!--  -->\n",
    "_Agent using random actions to land the lunarlander_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Setup\n",
    "These are the same packages as in the last exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install gymnasium\n",
    "!pip install \"gymnasium[box2d]\"\n",
    "!pip install matplotlib\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from typing import Iterable\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "from collections import namedtuple\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 DDPG\n",
    "Remember that for Q-learning, we have the following update:\n",
    "\n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r + \\gamma \\max_{a'}Q(s', a') - Q(s, a))$\n",
    "\n",
    "with discount factor $\\gamma$, learning rate $\\alpha$, reward $r$, sampled state $s$, sampled next state $s'$ and sampled action $a$. The $\\max$ operator enumerates through the finite number of discrete actions and chooses the action correcponding to the maximum $Q(s', a')$. The $\\max$ operator essentially represents the greedy policy for a discrete case. \n",
    "\n",
    "In continuous action settings there are infinite many actions and hence we need a new strategy for selecting actions. This is where the Actor network, $\\mu(s)$ comes into play. The actor is the explicit representation of the action selection strategy (or policy). It takes in a state $s$ as input and outputs an action $a$ which it believes will yield the highest $Q$ value in state $s$. (Do you see the resembelance to the $\\max$ operator in DQN?)\n",
    "\n",
    "So we can rewrite the Q-learning update with the actor:\n",
    "\n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r + \\gamma Q(s', \\mu(s')) - Q(s, a))$\n",
    "\n",
    "\n",
    "The critic can then be updated with the Meas Square Loss as before:\n",
    "\n",
    "$ L(\\theta^{Q}) = \\mathop{\\mathbb{E}}_{(s, a, r, s')\\sim D}([r + \\gamma Q(s', \\mu(s';\\theta^{\\mu}); \\theta^{Q}) - Q(s, a; \\theta^{Q})]^2 )$  \n",
    "\n",
    "where D is a dataset of sampled transitions and $\\theta^{Q}$ and $\\theta^{\\mu}$ are parameters of the critic and actor networks respectively.\n",
    "\n",
    "The Actor network's objective is to output actions that maximise the value of the Critic. Hence the loss function for the Actor is the negative of its objective which is the value of the Critic.\n",
    "\n",
    "$L(\\theta^{\\mu}) =  -\\mathop{\\mathbb{E}}_{(s)\\sim D}(Q(s, \\mu(s;\\theta^{\\mu}); \\theta^{Q}))$\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **DDPG Warm-up**\n",
    "1. Can you suggest a way to use DQN even in continuous action settings?\n",
    "2. What could be a potential short coming of your suggested approach above?\n",
    "3. In DQN, the Q network would only take the observation as input and output Q values for each of the actions, What should the input and outputs be for the Q network (Critic) in continuous settings?\n",
    "4. Actor's update uses the chain rule for derivatives, complete the following $\\frac{\\partial Q(s,a)}{\\partial \\theta^{\\mu}} =...$  _(Hint: DDPG paper Eq. 6)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "4. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Critic\n",
    "We are representing the critic with a deep neural network. \n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Implementing the Critic**\n",
    "In this exercise we are using a state-based (not image based) observations. \n",
    "Critic should take in both obsevation and action and output the Q value. \n",
    "\n",
    "The architecture consists of Linear layers and ReLu activations:\n",
    "- fc1: input:(obs_dim + action_dim),  ouput: 256\n",
    "- fc2: input: 256, output: 256\n",
    "- fc3: input: 256, output: 1\n",
    "\n",
    "obs_dim and action_dim can be determined by examining the env using ```env.observation_space.shape``` and ```env.action_space.shape```\n",
    "\n",
    "\n",
    "Implement this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int):\n",
    "        \"\"\"\n",
    "        Initialize the Critic network.\n",
    "        \n",
    "        :param obs_dim: dimention of the observations\n",
    "        :param num_actions: dimention of the actions\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__()\n",
    "        # TODO: your code\n",
    "\n",
    "    def forward(self, obs: torch.Tensor, action: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: your code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Actor \n",
    "The actor is also explicitly represented by a neural network. \n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Implementing the Actor**\n",
    "In this exercise we are using a state-based (not image based) observations. \n",
    "Actor should take in an observation and output an action.\n",
    "\n",
    "The architecture consists of Linear layers and ReLU activations:\n",
    "\n",
    "**NOTE**: The output of the last layer should pass through a tanh (not ReLU) activation and should be scaled and translated according to the environment's action space. Tanh activation restricts the acitons to [-1,1] range and scaling and translation makes sure that the action is within the bounds of the environment's action space.\n",
    "- fc1: input:(obs_dim),  ouput: 256\n",
    "- fc2: input: 256, output: 256\n",
    "- fc3: input: 256, output: action_dim\n",
    "\n",
    "action_high and action_low can be determined by examining the env using ```env.action_space.high``` and ```env.action_space.low```\n",
    "\n",
    "\n",
    "\n",
    "Implement this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, obs_dim: int, action_dim: int, action_low: np.array, action_high: np.array):\n",
    "        \"\"\"\n",
    "        Initialize the Actor network.\n",
    "        \n",
    "        :param obs_dim: dimention of the observations\n",
    "        :param num_actions: dimention of the actions\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__()\n",
    "        # We are registering scale and bias as buffers so they can be saved and loaded as part of the model.\n",
    "        # Buffers won't be passed to the optimizer for training!\n",
    "        self.register_buffer(\n",
    "            \"action_scale\", torch.tensor((action_high - action_low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        self.register_buffer(\n",
    "            \"action_bias\", torch.tensor((action_high + action_low) / 2.0, dtype=torch.float32)\n",
    "        )\n",
    "        \n",
    "        # TODO: your code\n",
    "\n",
    "    def forward(self, obs: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: your code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer\n",
    "\n",
    "We can reuse the Replay Buffer from the last exersice!\n",
    "\n",
    "The replay buffer stores transitions of the form $(s, a, r, s')$ with $s$ as the current state, the action $a$, the reward $r$, and the next state $s'$. The buffer can perform two operations:\n",
    "- **store**: During sampling we observe transitions and store them with ``buffer.store(...)``. However, the buffer only has a fixed size\n",
    "(as we cannot store an infinte amount of data). When reaching it, the oldest samples are overwritten first.\n",
    "- **sample**: For training, we want to sample a batch of transitions from our buffer via ``buffer.sample(...)``. The transitions are sampled uniformly and with replacement i.e. the same transition can be sampled more than once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size: int):\n",
    "        \"\"\"\n",
    "        Create the replay buffer.\n",
    "\n",
    "        :param max_size: Maximum number of transitions in the buffer.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.max_size = max_size\n",
    "        self.position = 0\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns how many transitions are currently in the buffer.\"\"\"\n",
    "        return len(self.data)\n",
    "\n",
    "    def store(self, obs: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, next_obs: torch.Tensor, terminated: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Adds a new transition to the buffer. When the buffer is full, overwrite the oldest transition.\n",
    "\n",
    "        :param obs: The current observation.\n",
    "        :param action: The action.\n",
    "        :param reward: The reward.\n",
    "        :param next_obs: The next observation.\n",
    "        :param terminated: Whether the episode terminated.\n",
    "        \"\"\"\n",
    "        if len(self.data) < self.max_size:\n",
    "            self.data.append((obs, action, reward, next_obs, terminated))\n",
    "        else:\n",
    "            self.data[self.position] = (obs, action, reward, next_obs, terminated)   \n",
    "        self.position = (self.position + 1) % self.max_size         \n",
    "\n",
    "    def sample(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions uniformly and with replacement. The respective elements e.g. states, actions, rewards etc. are stacked\n",
    "\n",
    "        :param batch_size: The batch size.\n",
    "        :returns: A tuple of tensors (obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch), where each tensors is stacked.\n",
    "        \"\"\"\n",
    "\n",
    "        obss, acts, rews, next_obss, terms = [], [], [], [], []\n",
    "        for _ in range(batch_size):\n",
    "            index = np.random.randint(0, len(self.data))\n",
    "            \n",
    "            o, a, r, no, t = self.data[index]\n",
    "            obss.append(torch.as_tensor(o))\n",
    "            acts.append(torch.as_tensor(a))\n",
    "            rews.append(torch.as_tensor(r))\n",
    "            next_obss.append(torch.as_tensor(no))\n",
    "            terms.append(torch.as_tensor(t))\n",
    "\n",
    "        # Stack\n",
    "        obs_batch = torch.stack(obss)\n",
    "        action_batch = torch.stack(acts)\n",
    "        reward_batch = torch.tensor(rews)\n",
    "        next_obs_batch = torch.stack(next_obss)\n",
    "        terminated_batch = torch.tensor(terms)\n",
    "\n",
    "        return obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Algorithm\n",
    "In this section, we will first look at the update of the DDPG and then implement the entire algorithm.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Implementing the DDPG Update**\n",
    "\n",
    "Write the functions ``update_critic`` and ``update_actor``, which gets the sampled data from the replay buffer, calculates the critic and actor loss and performs an update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_critic(\n",
    "        critic: nn.Module,\n",
    "        critic_target: nn.Module,\n",
    "        critic_optimizer: optim.Optimizer,\n",
    "        actor_target: nn.Module,\n",
    "        gamma: float,\n",
    "        obs: torch.Tensor,\n",
    "        act: torch.Tensor,\n",
    "        rew: torch.Tensor,\n",
    "        next_obs: torch.Tensor,\n",
    "        tm: torch.Tensor,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Update the DDPG's Critic network for one optimizer step.\n",
    "\n",
    "    :param critic: The critic network.\n",
    "    :param critic_target: The target critic network.\n",
    "    :param critic_optimizer: The critic's optimizer.\n",
    "    :param actor: The actor network.\n",
    "    :param actor_target: The target actor network.\n",
    "    :param actor_optimizer: The actor's optimizer.\n",
    "    :param gamma: The discount factor.\n",
    "    :param obs: Batch of current observations.\n",
    "    :param act: Batch of actions.\n",
    "    :param rew: Batch of rewards.\n",
    "    :param next_obs: Batch of next observations.\n",
    "    :param tm: Batch of termination flags.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: Critic Update\n",
    "\n",
    "def update_actor(critic: nn.Module,\n",
    "                 actor: nn.Module,\n",
    "                 actor_optimizer: optim.Optimizer,\n",
    "                 obs: torch.Tensor,\n",
    "                ):\n",
    "    \"\"\"\n",
    "    Update the DDPG's Actor network for one optimizer step.\n",
    "\n",
    "    :param critic: The critic network.\n",
    "    :param actor: The actor network.\n",
    "    :param actor_optimizer: The actor's optimizer.\n",
    "    :param obs: Batch of current observations.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: Actor Update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Polyak Update of the target networks\n",
    "It is common to update the target networks very slowly at every step. Implement the function below to implement this mechanism:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyak_update(\n",
    "    params: Iterable[torch.Tensor],\n",
    "    target_params: Iterable[torch.Tensor],\n",
    "    tau: float,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform a Polyak average update on ``target_params`` using ``params``:\n",
    "\n",
    "    :param params: parameters of the original network (model.parameters())\n",
    "    :param target_params: parameters of the target network (model_target.parameters())\n",
    "    :param tau: the soft update coefficient (\"Polyak update\", between 0 and 1) 1 -> Hard update, 0 -> No update\n",
    "    \"\"\"\n",
    "    # TODO: implement soft update. target_param <--- (1-tau)*target_param + tau*param\n",
    "    # Hint: You can use p.data.mul_ for inplace multiplication\n",
    "    # Hint: You can use torch.add(..., out=p.data) to directly put the result of the addition inside p.data\n",
    "    # p here is any torch.Tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now putting it all together. This implementation is very similar to the DQN one. \n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Implementing the DDPG Agent**\n",
    "\n",
    "Implement the DDPG agent by filling in the missing gaps in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
    "\n",
    "\n",
    "class DDPGAgent:\n",
    "    def __init__(self,\n",
    "            env,\n",
    "            exploration_noise=0.1,\n",
    "            gamma=0.99,\n",
    "            lr=0.001, \n",
    "            batch_size=64,\n",
    "            tau=0.005,\n",
    "            maxlen=100_000,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the DDPG agent.\n",
    "\n",
    "        :param env: The environment.\n",
    "        :param exploration_noise.\n",
    "        :param gamma: The discount factor.\n",
    "        :param lr: The learning rate.\n",
    "        :param batch_size: Mini batch size.\n",
    "        :param tau: Polyak update coefficient.\n",
    "        :param max_size: Maximum number of transitions in the buffer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "        self.exploration_noise = exploration_noise\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.tau = tau\n",
    "\n",
    "        # TODO: Initialize the Replay Buffer\n",
    "\n",
    "        # TODO: Initialize the Critic and Actor networks\n",
    "\n",
    "        # TODO: Initialze the target Critic and Actor networks and load the corresponding state_dicts\n",
    "        \n",
    "        # TODO: Create ADAM optimizer for the Critic and Actor networks\n",
    "\n",
    "\n",
    "    def train(self, num_episodes: int) -> EpisodeStats:\n",
    "        \"\"\"\n",
    "        Train the DDPG agent.\n",
    "\n",
    "        :param num_episodes: Number of episodes to train.\n",
    "        :returns: The episode statistics.\n",
    "        \"\"\"\n",
    "        # Keeps track of useful statistics\n",
    "        stats = EpisodeStats(\n",
    "            episode_lengths=np.zeros(num_episodes),\n",
    "            episode_rewards=np.zeros(num_episodes),\n",
    "        )\n",
    "        current_timestep = 0\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "            # Print out which episode we're on, useful for debugging.\n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                print(f'Episode {i_episode + 1} of {num_episodes}  Time Step: {current_timestep}')\n",
    "\n",
    "            # Reset the environment and get initial observation\n",
    "            obs, _ = self.env.reset()\n",
    "            \n",
    "            for episode_time in itertools.count():\n",
    "                # TODO: Use the Actor to choose an action\n",
    "\n",
    "                next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Update statistics\n",
    "                stats.episode_rewards[i_episode] += reward\n",
    "                stats.episode_lengths[i_episode] += 1\n",
    "\n",
    "                # TODO: Store sample in the replay buffer\n",
    "\n",
    "                # TODO: Sample a mini batch from the replay buffer\n",
    "                \n",
    "                # TODO: Update the Critic network\n",
    "\n",
    "                # TODO: Update the Actor network\n",
    "\n",
    "                # TODO: Update the target networks (Critic and Actor) via Polyak Update\n",
    "                \n",
    "                current_timestep += 1\n",
    "\n",
    "                # Check whether the episode is finished\n",
    "                if terminated or truncated or episode_time >= 500:\n",
    "                    break\n",
    "                obs = next_obs\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Training\n",
    "Now, we want to run our algorithm on a task in [MinAtar](https://github.com/kenjyoung/MinAtar). You are free to choose any environment you like, however, we recommend Breakout-v1 as the other environments may need different hyperparameters and more training time. The following game ID's are available: SpaceInvaders-v1, Breakout-v1, Seaquest-v1, Asterix-v1 and Freeway-v1.\n",
    "Note, that the training can take several minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose your environment\n",
    "env = gym.make(\"LunarLander-v2\",continuous = True, render_mode=\"rgb_array\")\n",
    "# Print observation and action space infos\n",
    "print(f\"Training on {env.spec.id}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\\n\")\n",
    "\n",
    "# Hyperparameters, Hint: Change as you see fit\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 8\n",
    "REPLAY_BUFFER_SIZE = 100_000\n",
    "TAU = 0.005\n",
    "EXPLORATION_NOISE=0.1\n",
    "NUM_EPISODES = 1_000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "# Train DDPG\n",
    "agent = DDPGAgent(\n",
    "    env, \n",
    "    exploration_noise=EXPLORATION_NOISE,\n",
    "    gamma=DISCOUNT_FACTOR,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    tau=TAU,\n",
    "    maxlen=REPLAY_BUFFER_SIZE,\n",
    ")\n",
    "stats = agent.train(NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving and loading the trained actor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the trained actor\n",
    "torch.save(agent.actor, \"ddpg_actor.pt\")\n",
    "\n",
    "# loading the trained actor\n",
    "loaded_actor = torch.load(\"ddpg_actor.pt\")\n",
    "loaded_actor.eval()\n",
    "print(loaded_actor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Results\n",
    "\n",
    "Like in the last exercise, we will look at the resulting episode reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_window=20\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "# Plot the episode length over time\n",
    "ax = axes[0]\n",
    "ax.plot(stats.episode_lengths)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Length\")\n",
    "ax.set_title(\"Episode Length over Time\") \n",
    "\n",
    "# Plot the episode reward over time\n",
    "ax = axes[1]\n",
    "rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "ax.plot(rewards_smoothed)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Reward (Smoothed)\")\n",
    "ax.set_title(f\"Episode Reward over Time\\n(Smoothed over window size {smoothing_window})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let us see what the learned policy does in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IImage\n",
    "\n",
    "def save_rgb_animation(rgb_arrays, filename, duration=50):\n",
    "    \"\"\"Save an animated GIF from a list of RGB arrays.\"\"\"\n",
    "    # Create a list to hold each frame\n",
    "    frames = []\n",
    "\n",
    "    # Convert RGB arrays to PIL Image objects\n",
    "    for rgb_array in rgb_arrays:\n",
    "        rgb_array = (rgb_array).astype(np.uint8)\n",
    "        img = Image.fromarray(rgb_array)\n",
    "        frames.append(img)\n",
    "    \n",
    "    # Save the frames as an animated GIF\n",
    "    frames[0].save(filename, save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
    "\n",
    "def rendered_rollout(policy, env, max_steps=1_000):\n",
    "    \"\"\"Rollout for one episode while saving all rendered images.\"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    imgs = [env.render()]\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        with torch.no_grad():\n",
    "            action = policy(torch.as_tensor(obs, dtype=torch.float32)).cpu().numpy()\n",
    "            \n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        imgs.append(env.render())\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return imgs\n",
    "\n",
    "imgs = rendered_rollout(loaded_actor, env)\n",
    "save_rgb_animation(imgs, \"trained.gif\")\n",
    "IImage(filename=\"trained.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not train very far because it can be very time-consuming.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Over Estimation Bias**\n",
    "\n",
    "1. One of the common issues with Q-Learning based algorithms is the Over Estimation Bias. Explain in detail what is over estimation bias and the reason behind it. \n",
    "2. One of the algorithms that closely resembles DDPG is (Twin Delayed Deep Deterministic policy gradient (TD3))[https://arxiv.org/pdf/1802.09477.pdf], what strategy did they employ to address the over estimation bias?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "1. ...\n",
    "2. ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
