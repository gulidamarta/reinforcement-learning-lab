{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2 - Q-learning\n",
    "In this exercise, we will train our own agent using tabular [Q-learning](https://www.youtube.com/watch?v=__t2XRxXGxI) to solve the [Cliff Walking task](https://gymnasium.farama.org/environments/toy_text/cliff_walking/). Q-learning is a popular method for model-free control. Our goal is to the learn the optimal action-value function $Q^*(s, a)$ for a given state $s$ and action $a$, by iteratively sampling from the environment and updating our current Q-estimate. To represent the Q-function, we use a table, where we store an estimate for each state and action.\n",
    "\n",
    "In contrast to behavior cloning, we do not have the correct actions available for our states. We only get a reward signal and have to actively search for the best behavior.\n",
    "\n",
    "<img src=\"cliff_walking.gif\" alt=\"Cliff Walking\" width=\"60%\"/>\n",
    "\n",
    "\n",
    "_Agent using random actions to play Cliff walking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Setup\n",
    "These are the same packages as in the last exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (2.1.1)\n",
      "Requirement already satisfied: torchvision in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (0.16.1)\n",
      "Requirement already satisfied: torchaudio in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: requests in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: gymnasium==0.29.1 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium==0.29.1) (1.26.2)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium==0.29.1) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium==0.29.1) (4.8.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium==0.29.1) (0.0.4)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from gymnasium==0.29.1) (6.8.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from importlib-metadata>=4.8.0->gymnasium==0.29.1) (3.17.0)\n",
      "Requirement already satisfied: minatar==1.0.15 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (1.0.15)\n",
      "Requirement already satisfied: cycler>=0.10.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (1.4.5)\n",
      "Requirement already satisfied: matplotlib>=3.0.3 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.16.2 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (1.26.2)\n",
      "Requirement already satisfied: pandas>=0.24.2 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (2.1.3)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2018.9 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (2023.3.post1)\n",
      "Requirement already satisfied: scipy>=1.2.1 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (1.11.4)\n",
      "Requirement already satisfied: seaborn>=0.9.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (0.13.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from minatar==1.0.15) (1.16.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (1.2.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (4.45.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (10.1.0)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib>=3.0.3->minatar==1.0.15) (6.1.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from pandas>=0.24.2->minatar==1.0.15) (2023.3)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib>=3.0.3->minatar==1.0.15) (3.17.0)\n",
      "Requirement already satisfied: matplotlib in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (3.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (4.45.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (10.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from matplotlib) (6.1.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: imageio in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (2.33.0)\n",
      "Requirement already satisfied: numpy in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from imageio) (1.26.2)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /Users/martagulida/opt/anaconda3/envs/rl_lab/lib/python3.9/site-packages (from imageio) (10.1.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install gymnasium==0.29.1\n",
    "!pip install minatar==1.0.15\n",
    "!pip install matplotlib\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable, Dict\n",
    "from PIL import Image\n",
    "from collections import defaultdict, namedtuple\n",
    "import itertools\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Reinforcement Learning\n",
    "\n",
    "In Reinforcement Learning (RL), there is an interaction loop between an agent making decisions and an environment, which represents the world the agent is interacting with. After the agent performs an action like moving left or right, the environment returns the next state e.g. the next frame in a video game, and a reward, a scalar value that tells the agent how well the current state is. For example, scoring points might earn the agent a positive reward.\n",
    "\n",
    "<img src=\"agent_env_interaction.svg\" alt=\"agent environment interaction\" width=\"40%\"/>\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **RL Warm-up**\n",
    "1. Describe a key difference between Reinforcement Learning and Deep Learning.  \n",
    "2. Define a Markov decision process (MDP).\n",
    "3. Explain the concept of a policy $\\pi$ in the context of RL. Given a policy, what does the action value $Q_{\\pi}(s,a)$ describe?\n",
    "4. What do we mean by the exploration and exploitation trade-off . Why can we not focus exclusively on one? \n",
    "5. What is the difference between model-based and model-free methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "4. ...\n",
    "5. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Gymnasium\n",
    "In Python, we can represent the RL loop using the [Gymnasium interface](https://gymnasium.farama.org/). In the following, we give a short introduction into the library Gymnasium.\n",
    "\n",
    "\n",
    "## 2.1 Interacting with the Environment\n",
    "First, we create our environment using the `gym.make()` command, specifying the environment's name as a string. In our example, we're using the Cliff Walking task, where an agent must walk to a goal without falling from the cliff. For every step the agent takes, he receives a reward of $-1$. When the agent falls of the cliff he must start from the beginning and receives a reward of $-100$.\n",
    "\n",
    "Once the environment is created, we reset it to the initial state with `env.reset()`. This function returns two values. The first is the initial state, which for our task is represented by an integer ranging from $0$ to $48$ describing all the positions the agent can be in as the game is played out on a $4\\times12$ map. Note, although we will refer to the state as `observation`, you can still think of both being the same for our task. The second value provides extra debugging information, but we will not be using it.\n",
    "\n",
    "Finally, we can visualize the current state using `env.render()`. Since we specified `render_mode=\"rgb_array\"`, this function returns an RGB image that we can display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = gym.make(\"CliffWalking-v0\", render_mode=\"rgb_array\")\n",
    "\n",
    "# Reset and the agent receives the first observation and additional info\n",
    "obs, info = env.reset()\n",
    "print(f\"Current state is: {obs}\") # Location [x=0, y=3] -> 0 + 3 * 12 = 36.\n",
    "\n",
    "# Render current state\n",
    "img = env.render()\n",
    "Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the behavior of our agent using a policy that takes the current observation (state) as input and gives us an action as output: $\\pi(s) = a$. Once we decided on an action using our policy, we can simulate the environment for one timestep using `env.step(action)`. This function returns the next observation and the reward for the action we took. Further variables are returned, like whether the environment terminated e.g. we reached the goal, if the episode was truncated, and some extra information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actions to control agent\n",
    "ACTIONS = {\n",
    "    0: \"MOVE UP\",\n",
    "    1: \"MOVE RIGHT\",\n",
    "    2: \"MOVE DOWN\",\n",
    "    3: \"MOVE LEFT\",\n",
    "}\n",
    "\n",
    "def random_policy(obs: int) -> int:\n",
    "    \"\"\"A policy that performs random actions.\"\"\"\n",
    "    return np.random.randint(4)\n",
    "\n",
    "# Simulate for two time steps\n",
    "TIME_STEPS = 2\n",
    "for i in range(TIME_STEPS):\n",
    "    action = random_policy(obs)\n",
    "    print(f\"Performing action: {ACTIONS[action]}\")\n",
    "    next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "    print(f\"Next state is: {next_obs}, reward: {reward}\")\n",
    "    obs = next_obs\n",
    "\n",
    "img = env.render()\n",
    "Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:orange\">**EXERCISE**</span>: **Environment Sampling**\n",
    "\n",
    "Implement the `sample` function, which simulates the environment for a specified number of time steps and returns the average undiscounted sum of rewards per finshed episode. Note, if `env.step` returns that either termination or truncation is true, the current episode ends, but the environment will not auto-reset itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(env: gym.Env, policy: Callable[[int], int], num_steps: int = 100_000) -> float:\n",
    "    \"\"\"\n",
    "    Samples for num_steps and returns the average undiscounted sum of rewards per episode.\n",
    "\n",
    "    :param env: The Cliff Walking environment. Note: may not have been reset.\n",
    "    :param policy: A function that maps from observations to actions.\n",
    "    :param num_steps: The number of steps to sample.\n",
    "    :returns: Average undiscounted return per episode, unfinished episodes are not counted.\n",
    "    \"\"\"\n",
    "    \n",
    "    # TODO: Your Code\n",
    "    ...\n",
    "\n",
    "undiscounted_return = sample(env, random_policy, num_steps=100_000)\n",
    "print(f\"The random policy has an average undiscounted return of {undiscounted_return}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Action and observation spaces\n",
    "Environments specify the formats of their valid actions and observations using `env.action_space` and `env.observation_space`, respectively. These fields use instances of the [Space](https://gymnasium.farama.org/api/spaces/) class, offering a variety of space definitions. Interesting for us are:\n",
    "- `Discrete(n=.., start=0)`: Discrete space, where the values are between `start` (inclusive) and `n` (exclusive).\n",
    "- `Box(low=.., high=.., shape=.., dtype=..)`: Either continuous or discrete space based on the dtype. The values are bound between `low` and `high` and have the specified shape.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print observation and action space of Cliff Walking\n",
    "env_cliff_walking = gym.make(\"CliffWalking-v0\")\n",
    "\n",
    "print(f\"Observation space of Cliff Walking Env: {env.observation_space}\")\n",
    "print(f\"Action space of Cliff Walking Env: {env.action_space}\")\n",
    "\n",
    "# Other environments also have these attributes\n",
    "env_breakout = gym.make('MinAtar/Breakout-v1') # from the last exercise\n",
    "\n",
    "print(f\"Observation space of Breakout Env: {env_breakout.observation_space}\")\n",
    "print(f\"Action space of Breakout Env: {env_breakout.action_space}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These attributes give us information about our environment and enable us to design policies and value functions in such a way that they are compatible with the environment.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Cliff Walking MDP**\n",
    "\n",
    "Formalize the Cliff Walking Environment as Markov Decision Process.\n",
    "Hint: You do not need to define the state transition and the reward function for each state individually, rather you can describe different scenarios and their rewards and probabilities. Also consider what happens if you are on the edge of the field or fall into the cliff.\n",
    "\n",
    "**Your answer:** ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Tabular Q-learning\n",
    "Now, we want to implement Q-learning. Before starting this section, it might be helpful to go back to slides of the lecture to read the core concepts behind Q-learning.\n",
    "\n",
    "### 3.1 Q-function\n",
    "We start with the representation of the Q-function. It maps state-action pairs to values. However, when implementing the Q-function, it is often implemented in a slightly different way. Normally we would assume that given a state and an action we get a single value, but we can also represent the Q-function as taking a state as input and returning a vector of values, each corresponding to an action. This is possible if we have a finite set of actions, and it can be very helpful if we want to maximize over the Q-values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q with all zeros\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "# Given an observation, we get an ndarray of values, with size equal to the number of actions\n",
    "state, info = env.reset()\n",
    "values = Q[state]\n",
    "\n",
    "print(f\"Q values for state {obs} are: {values}\")\n",
    "\n",
    "# As actions are integers starting from zero, we can still get single values via\n",
    "action = 0\n",
    "value = Q[state][action]\n",
    "\n",
    "print(f\"Q value for state {obs} and action {ACTIONS[action]} is: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Policy\n",
    "Next, we want to implement our policy, which is used to sample from the environment.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Implementing the $\\epsilon$-greedy policy**\n",
    "\n",
    "Implement the $\\epsilon$-greedy policy from the lecture, where an action is chosen at random with a probability of $\\epsilon$ and otherwise the best action according to our current Q-estimate. To do this, we have the function `make_epsilon_greedy_policy`, which returns a function `policy_fn` that implements the epsilon greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q: Dict[int, np.ndarray], epsilon: float, num_actions: int):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "\n",
    "    :param Q: A dictionary that maps from observation -> action-values. Each value is a numpy array of length num_actions (see below)\n",
    "    :param epsilon: The probability to select a random action.\n",
    "    :param num_actions: Number of actions in the environment.\n",
    "\n",
    "    :returns: A function that takes the observation as an argument and returns the greedy action in form of an int.\n",
    "    \"\"\"\n",
    "\n",
    "    def policy_fn(obs):\n",
    "        \"\"\"This function takes in the observation and returns an action.\"\"\"\n",
    "        # TODO: Implement everything inside policy_fn\n",
    "        ...\n",
    "\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Questions about the $\\epsilon$-greedy policy**\n",
    "1. Explain the function of the hyperparameter $\\epsilon$. What happens when it is set to zero or to one, respectively?\n",
    "2. Calculate the probability of selecting the optimal action according to our Q-function when $\\epsilon = 0.2$ and the number of actions is $4$. Hint: Consider the exploration as well!\n",
    "\n",
    "**Your answers:**\n",
    "1. ...\n",
    "2. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Algorithm\n",
    "Now we want to put everything together.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Implementing Q-learning**\n",
    "\n",
    "Implement the Q-learning algorithm by filling in the missing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
    "\n",
    "def q_learning(env: gym.Env, num_episodes: int, discount_factor=1.0, alpha=0.5, epsilon=0.1) -> (Dict[int, np.ndarray], EpisodeStats):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy while following an epsilon-greedy policy\n",
    "\n",
    "    :param env: the environment.\n",
    "    :param num_episodes: Number of episodes to run for.\n",
    "    :param discount_factor: Lambda time discount factor.\n",
    "    :param alpha: TD learning rate.\n",
    "    :param epsilon: The probability to select a random action.\n",
    "\n",
    "    :returns: A tuple (Q, episode_lengths). Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "      stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes),\n",
    "    )\n",
    "\n",
    "    # Initialize Q with all zeros\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # TODO: Create the epsilon-greedy policy\n",
    "    policy = ...\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if (i_episode + 1) % 100 == 0:\n",
    "            print(f'Episode {i_episode + 1} of {num_episodes}')\n",
    "\n",
    "        # TODO: Reset the environment and get initial observation\n",
    "        ...\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            # TODO: Choose action using our epsilon-greedy policy\n",
    "            action = ...\n",
    "\n",
    "            # TODO: Execute the action and observe next observation and reward\n",
    "            ...\n",
    "\n",
    "            # TODO: Implement the TD Update.\n",
    "            # Hint: Also think about whether the episode terminates! What is the Q-value of a terminal state?\n",
    "            ...\n",
    "            Q[obs][action] = ...\n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] += 1\n",
    "\n",
    "            # Check whether the episode is finished\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "            obs = next_obs\n",
    "\n",
    "    return Q, stats\n",
    "\n",
    "# Start the training\n",
    "Q, stats = q_learning(env, 500)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Convergence of Q-learning\n",
    "\n",
    "Given the `stats` of the last section, you can visualize the episode length and the epsiode reward, the agent achieved over training. The episode reward is the sum of all rewards collected in one episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_window=10\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "# Plot the episode length over time\n",
    "ax = axes[0]\n",
    "ax.plot(stats.episode_lengths)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Length\")\n",
    "ax.set_title(\"Episode Length over Time\")\n",
    "\n",
    "# Plot the episode reward over time\n",
    "ax = axes[1]\n",
    "rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "ax.plot(rewards_smoothed)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Reward (Smoothed)\")\n",
    "ax.set_title(f\"Episode Reward over Time\\n(Smoothed over window size {smoothing_window})\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can also look at how our policy behaves by plotting the action it would take for each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get policy\n",
    "policy = make_epsilon_greedy_policy(Q, epsilon=0., num_actions=env.action_space.n)\n",
    "\n",
    "# Define mapping for action representation\n",
    "ACTION_SYMBOLS = {\n",
    "    0: '↑',\n",
    "    1: '→',\n",
    "    2: '↓',\n",
    "    3: '←',\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "for x in range(12):\n",
    "    for y in range(4):\n",
    "        obs = x + 12 * y\n",
    "        action = policy(obs)\n",
    "        \n",
    "        # Set colors for specific boxes\n",
    "        box_color = 'white'\n",
    "        text_color = 'black'\n",
    "        if x == 0 and y == 3:\n",
    "            box_color = 'lightblue'\n",
    "        elif x == 11 and y == 3:\n",
    "            box_color = 'lightgreen'\n",
    "        elif y == 3:\n",
    "            box_color = 'grey'\n",
    "            text_color = box_color\n",
    "\n",
    "        plt.gca().add_patch(plt.Rectangle((x, 3-y), 1, 1, fill=True, color=box_color))\n",
    "        plt.text(x + 0.5, (3-y) + 0.5, ACTION_SYMBOLS[action], ha='center', va='center', color=text_color)\n",
    "\n",
    "plt.title('Policy for CliffWalking Environment')\n",
    "plt.xticks(range(12+1))\n",
    "plt.yticks(range(4+1))\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Interpreting training results**\n",
    "1. Did the training converge? Why do you think so or not?\n",
    "2. Explain why the episode reward does not converge smoothly towards the optimal value of $-13$. How could we fix that? \n",
    "\n",
    "**Your answers:**\n",
    "1. ...\n",
    "2. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let us see what the learned policy does in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IImage\n",
    "\n",
    "def save_rgb_animation(rgb_arrays, filename, duration=500):\n",
    "    \"\"\"Save an animated GIF from a list of RGB arrays.\"\"\"\n",
    "    # Create a list to hold each frame\n",
    "    frames = []\n",
    "\n",
    "    # Convert RGB arrays to PIL Image objects\n",
    "    for rgb_array in rgb_arrays:\n",
    "        img = Image.fromarray(rgb_array)\n",
    "        frames.append(img)\n",
    "\n",
    "    # Save the frames as an animated GIF\n",
    "    frames[0].save(filename, save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
    "\n",
    "def rendered_rollout(policy, env, max_steps=20):\n",
    "    \"\"\"Rollout for one episode while saving all rendered images.\"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    imgs = [env.render()]\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action = policy(obs)\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        imgs.append(env.render())\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    return imgs\n",
    "\n",
    "policy = make_epsilon_greedy_policy(Q, epsilon=0., num_actions=env.action_space.n)\n",
    "imgs = rendered_rollout(policy, env)\n",
    "save_rgb_animation(imgs, \"cliff_walking_trained.gif\")\n",
    "IImage(filename=\"cliff_walking_trained.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Going beyond tabular approaches**\n",
    " \n",
    "Why is tabular Q-learning not a feasible approach for high-dimensional problems? How could we represent our Q-function instead?\n",
    "\n",
    "\n",
    "**Your answer:** ..\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
