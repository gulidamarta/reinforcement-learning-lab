{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3 - DQN\n",
    "In this exercise we will train our own agent using [Deep Q Networks (DQN)](https://arxiv.org/pdf/1312.5602.pdf) to solve the tasks from [MinAtar](https://github.com/kenjyoung/MinAtar). Last time we looked at tabular Q-learning, which learns Q-values from sampled experience and stores its estimates in a table.  For high-dimensional problems, however, tabular methods become impractical, and we use function approximators such as neural networks instead.The DQN is a popular deep reinforcement learning method that does just that.It is based on Q-learning and uses deep neural networks as function approximators.\n",
    "\n",
    "<img src=\"breakout.gif\" alt=\"MinAtar\" width=\"20%\"/>\n",
    "\n",
    "\n",
    "_Agent using random actions to play MinAtar Breakout_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0 Setup\n",
    "These are the same packages as in the last exercise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio\n",
    "!pip install gymnasium==0.29.1\n",
    "!pip install minatar==1.0.15\n",
    "!pip install matplotlib\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from PIL import Image\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Callable\n",
    "from collections import namedtuple\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Deep Q-Networks\n",
    "In Q-learning, we have the following update:\n",
    "\n",
    "$Q(s,a) \\leftarrow Q(s,a) + \\alpha (r + \\gamma \\max_{a'}Q(s', a') - Q(s, a))$\n",
    "\n",
    "with discount factor $\\gamma$, learning rate $\\alpha$, reward $r$, sampled state $s$, sampled next state $s'$ and sampled action $a$.Instead of a table, we will now use a neural network to represent the Q-function $Q(s, a; \\theta)$, where $\\theta$ are the parameters of the neural network. In order to learn the Q-function, we minimize the Mean Squared Error loss between our current estimate and our TD target using Stochastic gradient descent (SGD):\n",
    "\n",
    "$ L(\\theta) = \\mathop{{}\\mathbb{E}}_{(s, a, r, s') \\sim D}([r + \\gamma \\max_{a'}Q(s', a'; \\theta') - Q(s, a; \\theta)]^2 )$  \n",
    "\n",
    "where D is a dataset of sampled transitions and $\\theta'$ are old parameters. This is very similar to how we train in a supervised learning setting!\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **DQN Warm-up**\n",
    "1. In the Q-update above, which part is the TD target and which one is the TD error? \n",
    "2. Given a neural network with one linear layer and a ReLU activation. Describe the parameters $\\theta$ of this network.\n",
    "3. Explain the difference between on and off-policy learning in RL. Q-learning is considered off-policy, why?\n",
    "4. In machine learning, we assume that the samples of our data set are independent and identically distributed (i.i.d.). What do we mean by this?\n",
    "5. What are minibatches and why are they used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "1. ...\n",
    "2. ...\n",
    "3. ...\n",
    "4. ...\n",
    "5. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Q-Network\n",
    "\n",
    "Since we are representing our Q-function as a deep neural network, we will first define it using PyTorch. Then we will define the epsilon greedy policy and introduce how to decay the epsilon over time.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Implementing the Q-Network**\n",
    "\n",
    "In the DQN paper, the network structure used is described as follows (although we have changed the hyperparameters!):\n",
    "> The input to the neural network consists [..] [of] an 10 × 10 × n image [...]. The first hidden layer convolves 16 5 × 5\n",
    "> filters with stride 1 with the input image and applies a rectifier nonlinearity. The second \n",
    "> hidden layer convolves 32 3 × 3 filters with stride 1, again followed by a rectifier nonlinearity. The\n",
    "> final hidden layer is fully-connected and consists of 128 rectifier units. The output layer is a fully-\n",
    "> connected linear layer with a single output for each valid action.\n",
    "\n",
    ">\n",
    "> <cite>from [Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602.pdf) (Mnih et al., 2013)</cite>\n",
    "\n",
    "Implement this architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, obs_shape: torch.Size, num_actions: int):\n",
    "        \"\"\"\n",
    "        Initialize the DQN network.\n",
    "        \n",
    "        :param obs_shape: Shape of the observation space\n",
    "        :param num_actions: Number of actions\n",
    "        \"\"\"\n",
    "\n",
    "        # obs_shape is the shape of a single observation -> use this information to define the dimensions of the layers\n",
    "        super(DQN, self).__init__()\n",
    "\n",
    "        # TODO: your code\n",
    "        ...\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # TODO: your code\n",
    "        ...\n",
    "\n",
    "        return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the epsilon greedy policy from our previous exercise. Note that we made some changes because Q is a network and epsilon is no longer fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(Q: nn.Module, num_actions: int):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "\n",
    "    :param Q: The DQN network.\n",
    "    :param num_actions: Number of actions in the environment.\n",
    "\n",
    "    :returns: A function that takes the observation as an argument and returns the greedy action in form of an int.\n",
    "    \"\"\"\n",
    "\n",
    "    def policy_fn(obs: torch.Tensor, epsilon: float = 0.0):\n",
    "        \"\"\"This function takes in the observation and returns an action.\"\"\"\n",
    "        if np.random.uniform() < epsilon:\n",
    "            return np.random.randint(0, num_actions)\n",
    "        \n",
    "        # For action selection, we do not need a gradient and so we call \".detach()\"\n",
    "        return Q(obs).argmax().detach().numpy()\n",
    "\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning of training we want high exploration, while towards the end we want high exploitation instead. As we use $\\epsilon$-greedy we can influence this tradeoff using the parameter of $\\epsilon$. However, setting it to a constant value will not have the desired effect, so we want to start at a high value and decrease it over time.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Linear $\\epsilon$-decay**\n",
    "\n",
    "Implement a linear decay function for $\\epsilon$, taking the start and end values as inputs. The function returns the current $\\epsilon$ based on the current time step and the duration. So after half the time has passed, the value of $\\epsilon$ should also be halfway between the start and end values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_epsilon_decay(eps_start: float, eps_end: float, current_timestep: int, duration: int) -> float:\n",
    "    \"\"\"\n",
    "    Linear decay of epsilon.\n",
    "\n",
    "    :param eps_start: The initial epsilon value.\n",
    "    :param eps_end: The final epsilon value.\n",
    "    :param current_timestep: The current timestep.\n",
    "    :param duration: The duration of the schedule (in timesteps). So when schedule_duration == current_timestep, eps_end should be reached\n",
    "\n",
    "    :returns: The current epsilon.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: your code\n",
    "    ...\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the linear epsilon decay works as intended. Your function should start at $\\epsilon=1.0$ and decrease linearly until it reaches time step $600$, where it reaches its final value of $\\epsilon=0.2$ and remains constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "eps_start = 1.0\n",
    "eps_end = 0.2\n",
    "schedule_duration = 1000\n",
    "\n",
    "eps_values = [linear_epsilon_decay(eps_start, eps_end, t, 600) for t in range(schedule_duration)]\n",
    "\n",
    "plt.plot(range(schedule_duration), eps_values)\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.title('Linear Epsilon Decay')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Target Network and Replay Buffer\n",
    "As described before, the main idea behind DQN is simple, we just minimize the MSE between the TD-target and the current estimate using the transitions we sampled. However, there are two problems that make the method very unstable:\n",
    "\n",
    "- **Non-stationary target**: The TD-target uses an estimate from our Q-network. Unlike supervised learning, this target is not fixed, and whenever we update our network parameters, the target changes as well.\n",
    "- Sampled transitions are **correlated** (each transition depends on the last transition if both are from the same episode). Samples are not independent.\n",
    "\n",
    "DQNs addresses both problems by using\n",
    "\n",
    "- **Target network:**\n",
    "An older set of network parameters is stored to compute the TD target, so they change less frequently and this improves stability. We call this network the target network. Its parameters are updated every few iterations.\n",
    "- **Experience Replay:**\n",
    "A buffer where all transitions are stored and randomly sampled to make the data distribution more stationary. The buffer has a fixed size and new samples overwrite old ones.\n",
    "\n",
    "First we look at the target network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "\n",
    "# Given a neural network\n",
    "net = nn.Sequential(\n",
    "    nn.Linear(4, 3),\n",
    "    nn.ReLU(), \n",
    "    nn.Linear(3, 1)\n",
    ")\n",
    "print(f\"Prediction (Before): {net(test_input)}\\n\")\n",
    "\n",
    "# We can get its parameters with .state_dict(). A dictionary containing all the parameters.\n",
    "# Note: It contains even more than parameters, but that is not relevant for us.\n",
    "params = copy.deepcopy(net.state_dict())\n",
    "\n",
    "print(\"Parameters:\")\n",
    "for key, value in params.items():\n",
    "    print(f\"- Parameter {key}:\\n{value}\")\n",
    "\n",
    "# Set weight matrix of first layer to zero\n",
    "net[0].weight.data.fill_(0.0)\n",
    "print(f\"\\nPrediction (After change): {net(test_input)}\\n\")\n",
    "\n",
    "# Load the old parameters\n",
    "net.load_state_dict(params)\n",
    "print(f\"Prediction (After reload): {net(test_input)}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can save and load old parameters of our network using ``.state_dict()`` and ``.load_state_dict(..)`` respectively.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Implement the Replay Buffer**\n",
    "\n",
    "The replay buffer stores transitions of the form $(s, a, r, s')$ with $s$ as the current state, the action $a$, the reward $r$, and the next state $s'$. The buffer can perform two operations:\n",
    "- **store**: During sampling we observe transitions and store them with ``buffer.store(...)``. However, the buffer only has a fixed size\n",
    "(as we cannot store an infinte amount of data). When reaching it, the oldest samples are overwritten first.\n",
    "- **sample**: For training, we want to sample a batch of transitions from our buffer via ``buffer.sample(...)``. The transitions are sampled uniformly and with replacement i.e. the same transition can be sampled more than once.\n",
    "\n",
    "In the following, implement the replay buffer by implementing both operations and also the ``___len___()`` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, max_size: int):\n",
    "        \"\"\"\n",
    "        Create the replay buffer.\n",
    "\n",
    "        :param max_size: Maximum number of transitions in the buffer.\n",
    "        \"\"\"\n",
    "        self.data = []\n",
    "        self.max_size = max_size\n",
    "        self.position = 0\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns how many transitions are currently in the buffer.\"\"\"\n",
    "        # TODO: Your code\n",
    "        return ...\n",
    "\n",
    "    def store(self, obs: torch.Tensor, action: torch.Tensor, reward: torch.Tensor, next_obs: torch.Tensor, terminated: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Adds a new transition to the buffer. When the buffer is full, overwrite the oldest transition.\n",
    "\n",
    "        :param obs: The current observation.\n",
    "        :param action: The action.\n",
    "        :param reward: The reward.\n",
    "        :param next_obs: The next observation.\n",
    "        :param terminated: Whether the episode terminated.\n",
    "        \"\"\"\n",
    "        # TODO: Your code\n",
    "        ...   \n",
    "\n",
    "    def sample(self, batch_size: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sample a batch of transitions uniformly and with replacement. The respective elements e.g. states, actions, rewards etc. are stacked\n",
    "\n",
    "        :param batch_size: The batch size.\n",
    "        :returns: A tuple of tensors (obs_batch, action_batch, reward_batch, next_obs_batch, terminated_batch), where each tensors is stacked.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: Your code\n",
    "        ...\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Replay Buffer**\n",
    "\n",
    "1. Suppose we have only one transition in the replay buffer and we are now sampling a batch of 16 samples. What happens and why might this be problematic for training?\n",
    "2. Imagine storing $100,000$ observations of the form $10 \\times 10 \\times 4$ with $32$ bits per entry. How many megabytes of memory will this require?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "1. ...\n",
    "2. ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Algorithm\n",
    "In this section, we will first look at the update of the DQN and then implement the entire algorithm.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Implementing the DQN Update**\n",
    "\n",
    "Write the function ``update_dqn``, which gets the sampled data from the replay buffer, calculates the DQN loss and performs an update step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dqn(\n",
    "        q: nn.Module,\n",
    "        q_target: nn.Module,\n",
    "        optimizer: optim.Optimizer,\n",
    "        gamma: float,\n",
    "        obs: torch.Tensor,\n",
    "        act: torch.Tensor,\n",
    "        rew: torch.Tensor,\n",
    "        next_obs: torch.Tensor,\n",
    "        tm: torch.Tensor,\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Update the DQN network for one optimizer step.\n",
    "\n",
    "    :param q: The DQN network.\n",
    "    :param q_target: The target DQN network.\n",
    "    :param optimizer: The optimizer.\n",
    "    :param gamma: The discount factor.\n",
    "    :param obs: Batch of current observations.\n",
    "    :param act: Batch of actions.\n",
    "    :param rew: Batch of rewards.\n",
    "    :param next_obs: Batch of next observations.\n",
    "    :param tm: Batch of termination flags.\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: Zero out the gradient\n",
    "    ...\n",
    "\n",
    "    # TODO: Calculate the TD-Target\n",
    "    with torch.no_grad():\n",
    "        td_target = ...\n",
    "\n",
    "    # TODO: Calculate the loss. Hint: Pytorch has the \".gather()\" function, which collects values along a specified axis using some specified indexes\n",
    "    loss = ...\n",
    "\n",
    "    # TODO: Backpropagate the loss and step the optimizer\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now putting it all together. This implementation is very similar to the Q-learning one. Note that we have not specified an environment yet! Our algorithm remains relatively flexible.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Implementing the DQN Agent**\n",
    "\n",
    "Implement the DQN agent by filling in the missing gaps in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EpisodeStats = namedtuple(\"Stats\", [\"episode_lengths\", \"episode_rewards\"])\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self,\n",
    "            env,\n",
    "            gamma=0.99,\n",
    "            lr=0.001, \n",
    "            batch_size=64,\n",
    "            eps_start=1.0,\n",
    "            eps_end=0.1,\n",
    "            schedule_duration=10_000,\n",
    "            update_freq=100,\n",
    "            maxlen=100_000,\n",
    "        ):\n",
    "        \"\"\"\n",
    "        Initialize the DQN agent.\n",
    "\n",
    "        :param env: The environment.\n",
    "        :param gamma: The discount factor.\n",
    "        :param lr: The learning rate.\n",
    "        :param batch_size: Mini batch size.\n",
    "        :param eps_start: The initial epsilon value.\n",
    "        :param eps_end: The final epsilon value.\n",
    "        :param schedule_duration: The duration of the schedule (in timesteps).\n",
    "        :param update_freq: How often to update the Q target.\n",
    "        :param max_size: Maximum number of transitions in the buffer.\n",
    "        \"\"\"\n",
    "\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.eps_start = eps_start\n",
    "        self.eps_end = eps_end\n",
    "        self.schedule_duration = schedule_duration\n",
    "        self.update_freq = update_freq\n",
    "\n",
    "        # TODO: Initialize the Replay Buffer\n",
    "        self.buffer = ...\n",
    "\n",
    "        # TODO: Initialize the Deep Q Network. Hint: Remember observation_space and action_space\n",
    "        self.q = ...\n",
    "\n",
    "        # TODO: Initialize the second Q Network, the target network. Load the parameters of the first one into the second\n",
    "        self.q_target = ...\n",
    "        ...\n",
    "\n",
    "        # TODO: Create an ADAM optimizer for the Q network\n",
    "        self.optimizer = ...\n",
    "\n",
    "        self.policy = make_epsilon_greedy_policy(self.q, env.action_space.n)\n",
    "\n",
    "\n",
    "    def train(self, num_episodes: int) -> EpisodeStats:\n",
    "        \"\"\"\n",
    "        Train the DQN agent.\n",
    "\n",
    "        :param num_episodes: Number of episodes to train.\n",
    "        :returns: The episode statistics.\n",
    "        \"\"\"\n",
    "        # Keeps track of useful statistics\n",
    "        stats = EpisodeStats(\n",
    "            episode_lengths=np.zeros(num_episodes),\n",
    "            episode_rewards=np.zeros(num_episodes),\n",
    "        )\n",
    "        current_timestep = 0\n",
    "        epsilon = self.eps_start\n",
    "\n",
    "        for i_episode in range(num_episodes):\n",
    "            # Print out which episode we're on, useful for debugging.\n",
    "            if (i_episode + 1) % 100 == 0:\n",
    "                print(f'Episode {i_episode + 1} of {num_episodes}  Time Step: {current_timestep}  Epsilon: {epsilon:.3f}  Transitions: {len(self.buffer)}')\n",
    "\n",
    "            # Reset the environment and get initial observation\n",
    "            obs, _ = self.env.reset()\n",
    "            \n",
    "            for episode_time in itertools.count():\n",
    "                # TODO: Get current epsilon value\n",
    "                epsilon = ...\n",
    "\n",
    "                # Choose action and execute\n",
    "                action = self.policy(torch.as_tensor(obs).unsqueeze(0).float(), epsilon=epsilon)\n",
    "                next_obs, reward, terminated, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Update statistics\n",
    "                stats.episode_rewards[i_episode] += reward\n",
    "                stats.episode_lengths[i_episode] += 1\n",
    "\n",
    "                # TODO: Store sample in the replay buffer\n",
    "                ...\n",
    "\n",
    "                # TODO: Sample a mini batch from the replay buffer\n",
    "                obs_batch, act_batch, rew_batch, next_obs_batch, tm_batch = ...\n",
    "                \n",
    "                # Update the Q network\n",
    "                update_dqn(\n",
    "                    self.q,\n",
    "                    self.q_target,\n",
    "                    self.optimizer,\n",
    "                    self.gamma, \n",
    "                    obs_batch.float(),\n",
    "                    act_batch, \n",
    "                    rew_batch.float(),\n",
    "                    next_obs_batch.float(),\n",
    "                    tm_batch\n",
    "                )\n",
    "\n",
    "                # Update the current Q target\n",
    "                if current_timestep % self.update_freq == 0:\n",
    "                    self.q_target.load_state_dict(self.q.state_dict())\n",
    "                current_timestep += 1\n",
    "\n",
    "                # Check whether the episode is finished\n",
    "                if terminated or truncated or episode_time >= 500:\n",
    "                    break\n",
    "                obs = next_obs\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Training\n",
    "Now, we want to run our algorithm on a task in [MinAtar](https://github.com/kenjyoung/MinAtar). As in the first exercise, we will train on the Breakout-v1 task. Feel free to change the hyperparameters or even use another environment from MinAtar. The following game ID's are available: SpaceInvaders-v1, Seaquest-v1, Asterix-v1 and Freeway-v1.\n",
    "Note, that the training can take several minutes and using another environment can take much more training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MinAtar/Breakout-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "# Print observation and action space infos\n",
    "print(f\"Training on {env.spec.id}\")\n",
    "print(f\"Observation space: {env.observation_space}\")\n",
    "print(f\"Action space: {env.action_space}\\n\")\n",
    "\n",
    "# Hyperparameters, Hint: Change as you see fit\n",
    "LR = 0.001\n",
    "BATCH_SIZE = 8\n",
    "REPLAY_BUFFER_SIZE = 100_000\n",
    "UPDATE_FREQ = 100\n",
    "EPS_START = 0.5\n",
    "EPS_END = 0.05\n",
    "SCHEDULE_DURATION = 15_000\n",
    "NUM_EPISODES = 1_000\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "\n",
    "# Train DQN\n",
    "agent = DQNAgent(\n",
    "    env, \n",
    "    gamma=DISCOUNT_FACTOR,\n",
    "    lr=LR,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    eps_start=EPS_START,\n",
    "    eps_end=EPS_END,\n",
    "    schedule_duration=SCHEDULE_DURATION,\n",
    "    update_freq=UPDATE_FREQ,\n",
    "    maxlen=REPLAY_BUFFER_SIZE,\n",
    ")\n",
    "stats = agent.train(NUM_EPISODES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.5 Results\n",
    "\n",
    "Like in the last exercise, we will look at the resulting episode reward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothing_window=20\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5), tight_layout=True)\n",
    "\n",
    "# Plot the episode length over time\n",
    "ax = axes[0]\n",
    "ax.plot(stats.episode_lengths)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Length\")\n",
    "ax.set_title(\"Episode Length over Time\") \n",
    "\n",
    "# Plot the episode reward over time\n",
    "ax = axes[1]\n",
    "rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "ax.plot(rewards_smoothed)\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_ylabel(\"Episode Reward (Smoothed)\")\n",
    "ax.set_title(f\"Episode Reward over Time\\n(Smoothed over window size {smoothing_window})\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let us see what the learned policy does in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image as IImage\n",
    "\n",
    "def save_rgb_animation(rgb_arrays, filename, duration=50):\n",
    "    \"\"\"Save an animated GIF from a list of RGB arrays.\"\"\"\n",
    "    # Create a list to hold each frame\n",
    "    frames = []\n",
    "\n",
    "    # Convert RGB arrays to PIL Image objects\n",
    "    for rgb_array in rgb_arrays:\n",
    "        rgb_array = (rgb_array*255).astype(np.uint8)\n",
    "        rgb_array = rgb_array.repeat(48, axis=0).repeat(48, axis=1)\n",
    "        img = Image.fromarray(rgb_array)\n",
    "        frames.append(img)\n",
    "\n",
    "    # Save the frames as an animated GIF\n",
    "    frames[0].save(filename, save_all=True, append_images=frames[1:], duration=duration, loop=0)\n",
    "\n",
    "def rendered_rollout(policy, env, max_steps=1_000):\n",
    "    \"\"\"Rollout for one episode while saving all rendered images.\"\"\"\n",
    "    obs, _ = env.reset()\n",
    "    imgs = [env.render()]\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        action = policy(torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0))\n",
    "        obs, _, terminated, truncated, _ = env.step(action)\n",
    "        imgs.append(env.render())\n",
    "        \n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    return imgs\n",
    "\n",
    "policy = make_epsilon_greedy_policy(agent.q, num_actions=env.action_space.n)\n",
    "imgs = rendered_rollout(policy, env)\n",
    "save_rgb_animation(imgs, \"trained.gif\")\n",
    "IImage(filename=\"trained.gif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did not train very far as it can be time-consuming.\n",
    "\n",
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **The deadly triad**\n",
    "\n",
    "In Sutton and Barto's \"Reinforcement Learning: An Introduction\" they introduced the concept of the **Deadly Triad**. Investigate this concept and explain in your own words what it describes. What does it say about DQNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<span style=\"color:orange\">**EXERCISE**</span>: **Going beyond discrete action spaces**\n",
    "\n",
    "1. What is the difference between a discrete and a continuous action space?\n",
    "2. Name one key problem that can arise when using DQN for continuous action spaces. Hint: Think about how we choose a greedy action in DQNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answers:**\n",
    "1. ...\n",
    "2. ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
